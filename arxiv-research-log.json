{
  "research_date": "2026-02-23",
  "searches": [
    {
      "query": "ArXiv cs.AI, cs.LG, cs.CL recent papers on memory structure evaluation, LLM agents, StructMemEval",
      "timestamp": "2026-02-23T00:46:00Z",
      "results_count": 1
    },
    {
      "query": "Configuration-to-Performance Scaling Law with Neural Ansatz",
      "timestamp": "2026-02-22T19:47:00Z",
      "results_count": 1
    },
    {
      "query": "multi-agent LLM frameworks chaos theory PDE solvers",
      "timestamp": "2026-02-22T20:13:00Z",
      "results_count": 7
    },
    {
      "query": "ArXiv cs.AI, cs.LG, cs.CL recent papers on LLM training, agent architectures, multi-agent systems",
      "timestamp": "2026-02-22T20:43:00Z",
      "results_count": 50
    },
    {
      "query": "Entropy-based adaptive guidance heterogeneous multi-agent systems",
      "timestamp": "2026-02-22T21:13:00Z",
      "results_count": 1
    },
    {
      "query": "ArXiv cs.AI recent papers on ODE-based LLM alignment, activation steering, control theory",
      "timestamp": "2026-02-22T21:43:00Z",
      "results_count": 1
    },
    {
      "query": "ArXiv cs.AI recent papers on long-horizon LLM agents, trajectory training, progressive RL",
      "timestamp": "2026-02-22T22:13:00Z",
      "results_count": 1
    },
    {
      "query": "ArXiv cs.AI recent papers on shifting optima in MARL, successive sub-value learning, Softmax-based exploration",
      "timestamp": "2026-02-22T23:13:00Z",
      "results_count": 1
    },
    {
      "query": "ArXiv cs.AI recent papers on Chain-of-Thought evaluation, reusability, verifiability, multi-agent IR",
      "timestamp": "2026-02-22T23:43:00Z",
      "results_count": 1
    },
    {
      "query": "ArXiv cs.AI, cs.CL recent papers on agent framework tabular discriminative classifiers",
      "timestamp": "2026-02-23T00:43:00Z",
      "results_count": 20
    },
    {
      "query": "LLM planning and reasoning benchmarks, long-horizon tasks, Wikipedia navigation",
      "timestamp": "2026-02-23T01:14:00Z",
      "results_count": 1
    },
    {
      "query": "Self-Evolving Multi-Agent Network for Industrial IoT Predictive Maintenance",
      "timestamp": "2026-02-23T01:43:00Z",
      "results_count": 1
    }
  ],
  "papers_read": [
    {
      "id": "2602.16738",
      "title": "Self-Evolving Multi-Agent Network for Industrial IoT Predictive Maintenance",
      "authors": [
        "Rebin Saleh",
        "Khanh Pham Dinh",
        "Balázs Villányi",
        "Truong-Son Hy"
      ],
      "submitted": "2026-02-17",
      "status": "Preprint, work in progress",
      "category": "Multi-Agent Systems, Industrial IoT, Edge-Fog-Cloud Architecture, Anomaly Detection",
      "relevance": "CRITICAL - First complete hierarchical multi-agent system designed for Industrial IoT predictive maintenance operations. Distributes specialized agents across Edge, Fog, and Cloud computational tiers. Self-evolving via PPO-based policy optimization. Achieves 8.6% F1 improvement over rule-based systems with 200-1500× latency reduction. LLM-based explainability achieves 82% operator acceptance.",
      "abstract_snippet": "Industrial IoT predictive maintenance requires systems capable of real-time anomaly detection without sacrificing interpretability or demanding excessive computational resources. Traditional approaches rely on static, offline-trained models that cannot adapt to evolving operational conditions, while LLM-based monolithic systems demand prohibitive memory and latency, rendering them impractical for on-site edge deployment. We introduce SEMAS, a self-evolving hierarchical multi-agent system that distributes specialized agents across Edge, Fog, and Cloud computational tiers. Edge agents perform lightweight feature extraction and pre-filtering; Fog agents execute diversified ensemble detection with dynamic consensus voting; and Cloud agents continuously optimize system policies via Proximal Policy Optimization (PPO) while maintaining asynchronous, non-blocking inference.",
      "notes": "First complete hierarchical multi-agent system for IIoT predictive maintenance. Three-layer architecture: Edge (4-8GB, O(1) computation), Fog (16-64GB, O(n) complexity), Cloud (unlimited, O(M) complexity, asynchronous). Self-evolving via PPO with trust-region constraints (ϵ=0.2) prevents oscillation. Consensus voting provides 6.5% F1 impact. Federated aggregation provides 2.0% F1 impact. LLM-based explainability achieves 82% operator acceptance. Achieves 8.6% F1 improvement over Baseline2 (rule-based) with statistical significance (t=4.21, p<0.001, Cohen's d=2.1). Latency: 1.22ms (Boiler), 0.30ms (Wind Turbine) vs Baseline1: 1923ms, 456ms. Strong connections to chaos theory (attractor dynamics, bifurcations, sensitive dependence, self-organization, entropy reduction), information theory (information bottleneck, mutual information, rate-distortion, channel capacity), control theory (hierarchical feedback control, Lyapunov stability, model-reference adaptive control), and brain theory (predictive coding, hierarchical processing, working memory vs long-term memory, information integration theory, cognitive load theory, dopamine reward systems, metacognition).",
      "key_contributions": [
        "Hierarchical multi-agent architecture for Edge-Fog-Cloud deployment with resource-aware agent placement",
        "Self-evolving policy optimization via PPO (Proximal Policy Optimization) with trust-region constraints preventing oscillation",
        "Collaborative ensemble detection with dynamic consensus voting (5-model ensemble: Isolation Forest, One-Class SVM, Local Outlier Factor, Elliptic Envelope, secondary Isolation Forest)",
        "Quantified multi-agent coordination benefits: Ablation studies confirm consensus voting (6.5% F1 impact), PPO optimization (3.5% F1 impact), and federated aggregation (2.0% F1 impact) each contribute materially",
        "LLM-based explainability for operator trust: Response generator achieves 82% operator acceptance vs 41% for numeric alerts",
        "200-1500× latency improvement enabling genuine real-time deployment (<100ms constraint)",
        "Comprehensive validation on two industrial datasets (Boiler Emulator: 10,000 samples, Wind Turbine: 500 samples)"
      ],
      "chaos_theory_connections": [
        "Attractor dynamics in detection space: Normal operations as stable attractors, anomalies as high-energy regions",
        "Sensitive dependence on initial conditions: Early Edge decisions (feature extraction) propagate through system (butterfly effect)",
        "Bifurcation analysis: Distribution shifts as bifurcation points; PPO tracks new attractors after bifurcations",
        "Self-organization: Multi-agent coordination spontaneously emerges distributed intelligence",
        "Consensus voting reduces entropy: Ensemble voting reduces system state space entropy by focusing on high-confidence detections",
        "PPO prevents limit cycles: Trust-region constraints (ϵ=0.2) prevent threshold oscillation, unlike rule-based systems",
        "Non-ergodic behavior: Training process exhibits non-ergodic exploration; stable convergence to optimal policy",
        "Information flow in hierarchical system: Edge→Fog (low entropy), Fog→Cloud (medium entropy), Cloud→Fog/Edge (low entropy optimization info)",
        "Layered attractor basins: Each computational tier as distinct attractor basin with different time scales"
      ],
      "information_theory_connections": [
        "Information bottleneck in hierarchical architecture: Edge compresses raw sensor data, Fog aggregates into anomaly scores, Cloud extracts into policies",
        "Mutual information maximization: Multi-agent coordination increases I(A₁, A₂; Y) - effective collaboration",
        "Rate-distortion tradeoff: Latency constraint (rate) vs accuracy requirement (distortion); SEMAS achieves 200-1500× speedup while maintaining accuracy",
        "Channel capacity via MQTT: Communication channels have finite capacity; QoS levels ensure reliable transmission",
        "Entropy reduction through consensus: Individual detector entropy H(a₁), H(a₂) reduced to H(a_fog) via ensemble voting",
        "LLM response generation as efficient encoding: LLM templates compress reasoning into natural language explanations",
        "Optimal feature selection: Edge feature extraction maximizes I(Features; Target) for downstream detection"
      ],
      "control_theory_connections": [
        "Three concurrent feedback mechanisms: Local feedback (Fog→Edge, <1s), Global feedback (Cloud→Fog/Edge), Iterative cycle (Fog↔Cloud)",
        "Lyapunov stability via PPO: Trust-region constraint ḣ(a) > 0 ensures policy update doesn't overshoot optimal",
        "Model-reference adaptive control: PPO continuously optimizes toward reference model (optimal F1-precision-recall balance)",
        "Hierarchical control with different time scales: High-level (Cloud: minutes), Mid-level (Fog: seconds), Low-level (Edge: milliseconds)",
        "Negative feedback for stabilization: Detection errors trigger Edge reconfiguration; F1 deviations trigger policy updates",
        "Gain scheduling via three guidance levels: Light (fast response), Moderate (balanced), Intensive (thorough exploration)",
        "Asynchronous non-blocking inference: Cloud policy optimization doesn't block real-time detection pipeline"
      ],
      "brain_theory_connections": [
        "Predictive coding hierarchy: Cloud (high-level predictions), Fog (mid-level detection), Edge (low-level feature extraction); Error signals propagate bottom-up",
        "Working memory vs long-term memory: Edge feature extraction (short-term cache), Fog detection (medium-term buffer), Cloud policies (consolidated long-term memory)",
        "Information Integration Theory (IIT): Multi-agent coordination creates high Φ (integrated information) through distributed information processing",
        "Cognitive load management: Edge (lightweight tasks O(1)), Fog (ensemble detection O(n)), Cloud (global optimization O(M), asynchronous)",
        "Hippocampus consolidation analog: Fog detection → Cloud policy optimization mimics hippocampus transferring to neocortex",
        "Metacognition via SHAP: Agent E computes SHAP values for policy validation and system oversight",
        "Dopamine reward system analogy: F1 improvement as reward signal; PPO policy gradient ascent as dopamine-modulated learning"
      ],
      "limitations": [
        "Training complexity: ~8 hours for hyperparameter optimization vs ~2 hours for Baseline1 (PPO, multi-agent coordination, multi-model ensemble)",
        "Memory overhead: 3.8GB vs 2.1GB (Baseline1) and 2.4GB (Baseline2) due to PPO experience replay buffer (10,000 transitions)",
        "Cold-start performance: First iteration depends on random initialization; no warm-start from pre-trained policies",
        "Dataset ceiling effect: Wind Turbine dataset high separability limits marginal benefit of adaptive mechanisms",
        "Explainability-latency trade-off: LLM response generation adds delay; problematic for frequent anomaly detection",
        "Hardware requirements: Requires GPU (A100 recommended) for training; Edge deployment requires 4-8GB RAM (feasible), Fog requires 16-64GB RAM (moderate), Cloud requires unlimited resources",
        "Hyperparameter sensitivity: Requires systematic grid search; though Table VIII provides justified values and sensitivity analysis"
      ],
      "future_directions": [
        "Transfer learning for rapid deployment: Pre-trained PPO policies for equipment classes (boiler-to-boiler, turbine-to-turbine)",
        "Multi-objective reward shaping: Simultaneously optimize F1 accuracy, false alarm cost, maintenance cost, energy consumption, downtime risk",
        "Federated learning with privacy guarantees: Current lightweight aggregation → differential privacy mechanisms (DP-SGD)",
        "Model compression for extreme edge: Quantization and knowledge distillation to deploy on <1GB RAM devices",
        "Cross-domain generalization: Validate transferability across industrial sectors (boiler → turbine → chemical reactor)",
        "Cached LLM explanations: Develop caching methods and minimal local explanation systems to reduce LLM response delay",
        "Extended validation: Beyond statistical significance testing, include robustness against distribution shift, adversarial inputs",
        "Dynamic adaptation: Explore adaptive step-size PPO, temperature-based exploration strategies",
        "Multi-agent extensions: Multi-strong + multi-weak configurations, hierarchical team structures, competitive scenarios",
        "Formal analysis: Convergence proofs for PPO, stability bounds, rate-distortion optimality proofs"
      ],
      "analyzed": true,
      "analysis_file": "/home/devbox/project/paper-2602.16738-analysis.md",
      "reproduction_guide": "/home/devbox/project/paper-2602.16738-reproduction-guide.md",
      "analysis_date": "2026-02-23"
    },
    {
      "id": "2602.11243",
      "title": "Evaluating Memory Structure in LLM Agents",
      "authors": [
        "Alina Shutova",
        "Alexandra Olenina",
        "Ivan Vinogradov",
        "Anton Sinitsin"
      ],
      "submitted": "2026-02-11",
      "status": "Preprint, work in progress",
      "category": "LLM Memory Systems, Evaluation Frameworks",
      "relevance": "HIGH - Introduces StructMemEval benchmark that tests agent's ability to organize long-term memory, not just factual recall. Reveals that modern LLMs do not always recognize memory structure without explicit prompting.",
      "abstract_snippet": "Modern LLM-based agents and chat assistants rely on long-term memory frameworks to store reusable knowledge, recall user preferences, and augment reasoning. As researchers create more complex memory architectures, it becomes increasingly difficult to analyze their capabilities and guide future memory designs. Most long-term memory benchmarks focus on simple fact retention, multi-hop recall, and time-based changes. While undoubtedly important, these capabilities can often be achieved with simple retrieval-augmented LLMs and do not test complex memory hierarchies. To bridge this gap, we propose StructMemEval - a benchmark that tests the agent's ability to organize its long-term memory, not just factual recall.",
      "notes": "Proposes StructMemEval benchmark focusing on memory organization (transaction ledgers, to-do lists, tree structures) rather than just factual recall. Key findings: (1) Simple RAG struggles with structured tasks, (2) Memory agents succeed when prompted how to organize memory, (3) LLMs don't automatically recognize memory structure without explicit prompts. Strong connections to chaos theory (memory state space as dynamical system, attractor dynamics, bifurcations), information theory (information bottleneck, mutual information, entropy reduction), control theory (feedback control, Lyapunov stability), and brain theory (predictive coding, hierarchical processing, working vs long-term memory, hippocampus consolidation).",
      "key_contributions": [
        "StructMemEval benchmark - focuses on memory organization not just factual recall",
        "Task types: transaction ledgers, to-do lists, tree structures",
        "Key finding 1: Simple RAG struggles with structured memory tasks",
        "Key finding 2: Memory agents succeed when given explicit organization prompts",
        "Key finding 3: LLMs don't automatically recognize memory structure without prompting",
        "Highlights important direction for LLM training and memory framework improvements"
      ],
      "chaos_theory_connections": [
        "Memory state space as high-dimensional dynamical system",
        "Structured memory as stable attractors - efficient retrieval",
        "Unstructured memory as chaotic - high entropy, unpredictable",
        "Memory reorganization as bifurcation point in dynamics",
        "Evolution trajectories in memory space",
        "Entropy reduction as self-organization from chaos to order"
      ],
      "information_theory_connections": [
        "Information bottleneck theory - memory as information storage channel",
        "Mutual information I(M;R) measures memory effectiveness",
        "Rate-distortion tradeoff - storage efficiency vs retrieval accuracy",
        "Conditional entropy H(M|R) - retrieval uncertainty",
        "Source coding - optimal encoding schemes for memory entries"
      ],
      "control_theory_connections": [
        "Feedback control - retrieval failures as error signals drive reorganization",
        "Lyapunov stability - well-organized memory as stable attractors",
        "Adaptive control - dynamic adjustment of retrieval strategies"
      ],
      "brain_theory_connections": [
        "Predictive coding - memory supports predictive retrieval",
        "Hierarchical processing - working memory, episodic memory, semantic memory",
        "Hippocampus consolidation - temporary to long-term memory transfer",
        "Information Integration Theory (IIT) - high Φ in well-organized memory",
        "Metacognition - self-monitoring of memory state"
      ],
      "limitations": [
        "Task types limited to 4 structures",
        "Lack of standardized evaluation framework",
        "Synthetic vs real-world tasks",
        "LLM inherent limitations (context window, hallucinations)",
        "Prompt dependence - memory agents need explicit organization prompts"
      ],
      "future_directions": [
        "Extend benchmark with more task types",
        "Real-world data integration",
        "Structure-aware LLM training",
        "Adaptive memory organization",
        "Cross-session continuity",
        "Metamemory support",
        "Formal analysis and optimization proofs"
      ],
      "analyzed": true,
      "analysis_file": "/home/devbox/project/paper-2602.11243-analysis.md",
      "reproduction_guide": "/home/devbox/project/paper-2602.11243-reproduction-guide.md",
      "analysis_date": "2026-02-23"
    },
    {
      "id": "2602.10300",
      "title": "Configuration-to-Performance Scaling Law with Neural Ansatz",
      "authors": [
        "Kaiyue Wen",
        "Tengyu Ma"
      ],
      "submitted": "2026-02-10",
      "category": "LLM Training Frameworks & Scaling Laws",
      "relevance": "CRITICAL - Revolutionary approach using LLM as neural ansatz to map full training configurations to performance, achieving 20-40% lower prediction error than Chinchilla law",
      "abstract_snippet": "We propose learning a Configuration-to-Performance Scaling Law (CPL): a mapping from full training configuration to training performance. Because no simple functional form can express this mapping, we parameterize it with a large language model (LLM), and fit it with diverse open-source pretraining logs.",
      "notes": "Achieves 20-40% lower prediction error. Uses Qwen3-1.7B as regressor. Deep connections to chaos theory (attractor basins, bifurcations), information theory (rate-distortion, mutual information), and brain theory (predictive coding, hierarchical processing).",
      "analyzed": true,
      "analysis_file": "/home/devbox/project/paper-2602.10300-analysis.md",
      "reproduction_guide": "/home/devbox/project/paper-2602.10300-reproduction-guide.md",
      "analysis_date": "2026-02-22"
    },
    {
      "id": "2602.17607",
      "title": "AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing",
      "authors": [
        "Jianda Du",
        "Youran Sun",
        "Haizhao Yang"
      ],
      "institution": "University of Maryland",
      "submitted": "2026-02-19",
      "category": "Multi-Agent Systems & PDE Solvers",
      "relevance": "HIGH - Multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language descriptions",
      "abstract_snippet": "We introduce AutoNumerics, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language descriptions. Unlike black-box neural solvers, our framework generates transparent solvers grounded in classical numerical analysis.",
      "notes": "Achieves 6 orders of magnitude lower error than CodePDE and neural baselines. Uses coarse-to-fine execution strategy and residual-based self-verification. Evaluates on 24 PDEs spanning 1D-5D. Strong connections to chaos theory (PDEs as dynamical systems, attractor dynamics), control theory (feedback mechanisms, coarse-to-fine hierarchy), and information theory (information compression, mutual information in scheme selection).",
      "key_contributions": [
        "Multi-agent framework with specialized LLM agents (Formulator, Planner, Feature, Selector, Coder, Critic, Reasoning)",
        "Coarse-to-fine execution strategy decoupling logic debugging from stability validation",
        "Residual-based self-verification for problems without analytic solutions",
        "200-PDE benchmark suite with evaluation on 24 representative problems",
        "Achieves relative L2 errors down to 10^-16 (near machine precision) for Poisson and Helmholtz"
      ],
      "chaos_theory_connections": [
        "PDEs as mathematical tools for dynamical systems (Navier-Stokes, KdV, Reaction-Diffusion)",
        "Multi-agent coordination as complex dynamical system with emergent behavior",
        "Scheme selection as attractor dynamics in solution space",
        "Feedback mechanisms (coarse-to-fine, residual verification, Fresh Restart) as control theory loops",
        "Scale hierarchy (coarse to fine) analogous to renormalization group and hierarchical information processing"
      ],
      "brain_theory_connections": [
        "Predictive coding: Planner Agent predicts effective schemes, Critic validates",
        "Information integration: High Φ (integrated information) due to multi-agent coordination",
        "Differentiation and integration: Specialized agents vs unified pipeline",
        "Hierarchical processing: Coarse logic → fine accuracy"
      ],
      "limitations": [
        "Limited accuracy on high-dimensional (≥5D) and high-order PDEs",
        "Only regular domains evaluated",
        "Coupled to single LLM (GPT-4.1)",
        "Lack formal convergence or stability guarantees"
      ],
      "future_directions": [
        "Formal analysis: Convergence proofs, stability bounds, rate-distortion analysis",
        "Cross-method generalization: Testing S2Q with other CTDE methods (VDN, QPLEX)",
        "Scalability improvements: Reducing computational overhead of encoder-decoder; approximate Softmax computation",
        "Real-world applications: Applying to robotics swarm control, autonomous vehicles, drone fleets",
        "Cross-disciplinary: Comparing to neuromodulation in biological brains, motion planning in robotics"
      ],
      "analyzed": true,
      "analysis_file": "/home/devbox/project/paper-2602.17607-analysis.md",
      "reproduction_guide": "/home/devbox/project/paper-2602.17607-reproduction-guide.md",
      "analysis_date": "2026-02-22"
    },
    {
      "id": "2602.17588",
      "title": "Modeling Distinct Human Interaction in Web Agents",
      "authors": [
        "Faria Huq",
        "et al."
      ],
      "submitted": "2026-02-19",
      "category": "Agent Architectures & Human-AI Collaboration",
      "relevance": "HIGH - Introduces principled approach to modeling human intervention patterns in autonomous web agents, identifying four distinct interaction patterns and developing intervention-aware models that achieve 26.5% improvement in user-rated usefulness",
      "abstract_snippet": "We introduce a task of modeling human intervention to support collaborative web task execution. We collect CowCorpus, a dataset of 400 real-user web navigation trajectories containing over 4,200 interleaved human and agent actions. We identify four distinct patterns of user interaction with agents -- hands-off supervision, hands-on oversight, collaborative task-solving, and full user takeover.",
      "notes": "Collects CowCorpus (400 trajectories, 4,200+ actions). Identifies four interaction patterns: hands-off (8-12% intervention), hands-on (40-50%), collaborative (25-35%), takeover (75-85%). Achieves 61.4-63.4% accuracy improvement in intervention prediction. Strong connections to chaos theory (attractor dynamics, bifurcations, coupled oscillators), control theory (negative feedback, Lyapunov stability), information theory (mutual information, information bottleneck), and brain theory (predictive coding, free energy principle, hierarchical processing).",
      "key_contributions": [
        "CowCorpus dataset: 400 real-user web navigation trajectories, 4,200+ interleaved actions",
        "Four distinct interaction patterns: hands-off supervision, hands-on oversight, collaborative task-solving, full user takeover",
        "Intervention prediction models: 61.4-63.4% accuracy improvement over base LMs",
        "User study: 26.5% increase in user-rated agent usefulness",
        "Demonstrates that structured modeling leads to more adaptive, collaborative agents"
      ],
      "chaos_theory_connections": [
        "Human-agent interaction as dynamical system with multiple attractors (four interaction patterns)",
        "Bifurcation analysis: transitions between interaction modes as system parameter changes",
        "Coupled oscillators analogy: human and agent as coupled oscillators with coupling strengths ε_H, ε_A",
        "Sensitive dependence on initial conditions: small changes in task difficulty or trust lead to large differences in interaction patterns",
        "Lyapunov stability: intervention as negative feedback that stabilizes agent behavior toward human preferences",
        "Phase space structure: each interaction pattern as distinct attractor basin in joint human-AI state space"
      ],
      "control_theory_connections": [
        "Intervention as negative feedback: humans provide corrective feedback to stabilize agent behavior",
        "Feedback control system architecture: Reference (human intent) → Controller (human) → Plant (agent) → Output (task execution) → Sensor (observation)",
        "Error detection: humans detect deviations from intent, generate error signals",
        "Corrective action: direct correction, preference clarification, task redefinition",
        "Lyapunov stability: hands-off mode should be designed to be Lyapunov stable with respect to human intent",
        "Bifurcation analysis: transition from hands-off to hands-on represents loss of stability (or shrinking stability basin)"
      ],
      "information_theory_connections": [
        "Mutual information I(H;A): measures alignment and understanding between human and agent",
        "Information bottleneck principle: cognitive capacity limits information transmission, optimal compression of human interventions",
        "Implicit vs. explicit information: action patterns, timing, no-intervention vs. verbal instructions",
        "Channel capacity: human communication bandwidth limited by cognitive load",
        "Entropy reduction: interventions reduce system entropy by ordering agent behavior"
      ],
      "brain_theory_connections": [
        "Predictive coding: humans generate predictions about agent behavior, compare to observations, update internal models",
        "Prediction error ε_t = A_{t+1}^{observed} - A_{t+1}^{predicted}",
        "Free energy principle: intervention minimizes variational free energy by updating beliefs (perceptual inference) or changing behavior (active inference)",
        "Hierarchical predictive coding: humans (high level: goals, strategy), agents (low level: specific actions)",
        "Intervention trigger: when prediction error exceeds threshold θ, human intervenes",
        "Free energy levels: hands-off (low), hands-on (high), collaborative (intermediate)"
      ],
      "limitations": [
        "Domain specificity: only tested on web navigation tasks, generalization unknown",
        "User demographics: limited diversity in study participants",
        "Agent model: single-agent focus, no multi-AI-agent collaboration explored",
        "Interaction granularity: binary intervention model (intervene vs. not interven), does not capture quality or effectiveness",
        "No formal proof of optimality for four interaction patterns"
      ],
      "future_directions": [
        "Cross-domain generalization: test on code generation, scientific research, creative work",
        "Multi-agent extension: scenarios with multiple AI agents, study emergent coordination",
        "Temporal dynamics: state transition models (Markov chains, HMMs), time-varying interaction patterns",
        "Chaos theory experiments: systematically test sensitivity to initial conditions, compute Lyapunov exponents",
        "Information theory analysis: compute mutual information in real data, test information bottleneck principle",
        "Control theory formalization: analyze Lyapunov stability, design optimal control policies"
      ],
      "analyzed": true,
      "analysis_file": "/home/devbox/project/paper-2602.17588-analysis.md",
      "reproduction_guide": "/home/devbox/project/paper-2602.17588-reproduction-guide.md",
      "analysis_date": "2026-02-22"
    },
    {
      "id": "2602.13639",
      "title": "Guided Collaboration in Heterogeneous LLM-Based Multi-Agent Systems via Entropy-Based Understanding Assessment and Experience Retrieval",
      "authors": [
        "Linlin Wang",
        "Tianqing Zhu",
        "Laiqiao Qin",
        "Longxiang Gao",
        "Wanlei Zhou"
      ],
      "institution": "City University of Macau, Macao, China",
      "submitted": "2026-02-14",
      "category": "Multi-Agent Systems, Information Theory, Control Theory",
      "relevance": "CRITICAL - Identifies and solves the negative synergy effect in heterogeneous multi-agent systems where strong-weak collaboration can underperform weak-weak combinations through an entropy-based adaptive guidance framework",
      "abstract_snippet": "With recent breakthroughs in large language models (LLMs) for reasoning, planning, and complex task generation, artificial intelligence systems are transitioning from isolated single-agent architectures to multi-agent systems with collaborative intelligence. However, in heterogeneous multi-agent systems (HMAS), capability differences among agents gives rise to consistent cognitive problems, where strong and weak models fail to contribute effectively. Through comprehensive experiments, we disclose a counterintuitive phenomenon in the strong-weak system: a strong-weak collaboration may under-perform weak-weak combinations, revealing that cognitive mismatching are key bottlenecks limiting heterogeneous cooperation.",
      "notes": "Identifies negative synergy effect: SW < WW in multiple cases (GSM8K: 45.0% vs. 52.0%, MBPP: 36.5% vs. 40.0%). Proposes entropy-based adaptive guidance framework with 5-dimensional entropy metrics (expression, uncertainty, structure, coherence, relevance), 3-level guidance system (light, moderate, intensive), and RAG-based experience retention. Strong connections to chaos theory (non-linear behavior, bifurcations, sensitive dependence), information theory (entropy as uncertainty metric, information bottleneck, mutual information), control theory (negative feedback loop, Lyapunov stability, adaptive control), and brain theory (predictive coding, cognitive load theory, IIT, episodic memory).",
      "key_contributions": [
        "First systematic identification of negative synergy effect in HMAS",
        "Multi-dimensional entropy assessment mechanism (5 components: expression, uncertainty, structure, coherence, relevance)",
        "Three-level adaptive guidance strategies with dynamic threshold adjustment",
        "RAG-based experience retention for long-term learning",
        "Consistent improvements across three benchmarks (GSM8K, MBPP, CVRP)"
      ],
      "negative_synergy_effect": {
        "gsm8k": "SW = 45.0% vs. WW = 52.0% (7.0% decrease)",
        "mbpp": "SW = 36.5% vs. WW = 40.0% (3.5% decrease)",
        "cvrp": "SW = 65.0% vs. WW = 69.3% (4.3% decrease)"
      },
      "chaos_theory_connections": [
        "Negative synergy effect as bifurcation: SW combination can perform WORSE than WW (non-monotonic behavior)",
        "Phase space interpretation: Each collaboration configuration (SS, WW, SW) as distinct attractors",
        "Sensitive dependence: Small differences in agent capabilities → large differences in collaboration outcomes",
        "Butterfly effect: Small perturbations in task formulation cause different trajectories",
        "Entropy as disorder: High entropy = high uncertainty = disordered cognitive state",
        "Self-organization: Framework drives system toward low-entropy (high understanding) states"
      ],
      "information_theory_connections": [
        "Shannon entropy core: Multi-dimensional entropy captures semantic, structural, pragmatic aspects of understanding",
        "Information transfer asymmetry: Strong→Weak has high information density but low transfer rate (bottleneck)",
        "Mutual information: I(Strong;Weak) measures effective collaboration; framework increases I(Strong;Weak)",
        "Information bottleneck: Weak agents have limited cognitive capacity; framework finds optimal compression",
        "Rate-distortion theory: Guidance as compression of strong agent's reasoning into absorbable form"
      ],
      "control_theory_connections": [
        "Negative feedback loop: Weak response → entropy calc → guidance adjustment → weak receives guidance → repeat",
        "Lyapunov stability: Light guidance should be stable; dynamic thresholds ensure asymptotic stability",
        "Adaptive control: Model-reference adaptive control with learning rate λ = 0.2 and max adjustment A_max = 0.6",
        "Gain scheduling: Three guidance levels as different control gains selected based on operating point",
        "Hierarchical control: High-level (RAG), mid-level (threshold adjustment), low-level (per-round assessment)"
      ],
      "brain_theory_connections": [
        "Predictive coding: Weak agent generates predictions, strong agent provides error signal (guidance)",
        "Hierarchical predictive coding: High-level (strong agent) provides coarse predictions, low-level (weak agent) generates fine-grained predictions",
        "Cognitive load theory: Entropy serves as proxy for cognitive load; framework manages load via adaptive guidance",
        "Information Integration Theory (IIT): Multi-dimensional entropy integration measures information integration (Φ)",
        "Episodic memory: RAG-based experience storage as episodic memory; retrieval as memory access",
        "Learning and consolidation: Immediate guidance = working memory, RAG = long-term memory consolidation",
        "Habit formation: Dynamic threshold adjustment represents learning curves; thresholds decrease with repeated success"
      ],
      "limitations": [
        "Limited agent pool: Only 4 LLMs (GPT-4o, Claude-3.5, Qwen-2.5-1B, LLaMA-3.2-3B)",
        "Task diversity: Only 3 types (math, code, routing); no creative/multi-modal/open-ended tasks",
        "Number of agents: Only 2-agent configurations; no multi-strong + multi-weak analysis",
        "Dynamic environments: All tasks static; no time-varying or adversarial scenarios",
        "Formal guarantees: No convergence proofs, stability analysis, or optimality bounds",
        "Entropy metric design: Heuristic 5-component design; task-dependent weights empirically set"
      ],
      "future_directions": [
        "Formal analysis: Convergence proofs, stability bounds, rate-distortion analysis",
        "Alternative entropy: Rényi/Tsallis entropy, cross-entropy, semantic entropy",
        "Larger agent pools: More open-source models, domain-specific models, different architectures",
        "Multi-agent configurations: Multi-strong + multi-weak, hierarchical team structures, competitive scenarios",
        "Task diversity: Creative tasks, multi-modal tasks, open-ended tasks, dynamic environments",
        "Real-world applications: Human-AI collaboration, educational systems, industrial automation, scientific collaboration",
        "Efficiency improvements: Real-time entropy calculation, approximate metrics, distributed calculation",
        "Meta-learning: Learn optimal weights, meta-learn guidance strategies, transfer across domains"
      ],
      "analyzed": true,
      "analysis_file": "/home/devbox/project/paper-2602.13639-analysis.md",
      "reproduction_guide": "/home/devbox/project/paper-2602.13639-reproduction-guide.md",
      "analysis_date": "2026-02-22"
    },
    {
      "id": "2602.17560",
      "title": "ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment",
      "authors": [
        "Hongjue Zhao",
        "Haosen Sun",
        "Jiangtao Kong",
        "Xiaochang Li",
        "Qineng Wang",
        "Liwei Jiang",
        "Qi Zhu",
        "Tarek Abdelzaher",
        "Yejin Choi",
        "Manling Li",
        "Huajie Shao"
      ],
      "institution": "UIUC, Northwestern University, William & Mary, University of Washington, Stanford University",
      "submitted": "2026-02-19",
      "conference": "ICLR 2026",
      "category": "LLM Training/Inference & Control Theory",
      "relevance": "CRITICAL - Novel framework for LLM alignment that reformulates activation steering as solving ordinary differential equations (ODEs) guided by barrier functions from control theory",
      "abstract_snippet": "Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: (i) lack of a unified theoretical framework for guiding the design of steering directions, and (ii) an over-reliance on one-step steering that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based theoretical framework for activation steering in LLM alignment.",
      "notes": "Achieves 5.7% improvement on TruthfulQA, 2.5% on UltraFeedback, 2.4% on RealToxicityPrompts. First paper to unify activation steering methods under ODE framework. Uses barrier functions from control theory to define desirable activation regions. Deep connections to chaos theory (ODEs as dynamical systems, attractor dynamics, bifurcations), control theory (barrier functions, Lyapunov stability, feedback control), information theory (log-density ratios, mutual information, Fisher information), and brain theory (predictive coding, free energy principle, hierarchical processing).",
      "key_contributions": [
        "Unified ODE-based theoretical framework: Interprets activation addition as Euler discretization of ODE",
        "Barrier function unification: Input reading and output optimization reinterpreted as barrier function design",
        "ODESteer method: Multi-step adaptive steering by numerically solving ODE guided by nonlinear barrier functions",
        "Polynomial Count Sketch: Efficient nonlinear feature mapping for density ratio estimation",
        "Consistent improvements across 4 models and 3 benchmarks with simple ML tools (logistic regression)"
      ],
      "chaos_theory_connections": [
        "ODEs as dynamical systems: Activation steering as trajectory design in high-dimensional phase space",
        "Attractor dynamics: Barrier functions define basins of attraction (desirable activation regions)",
        "Numerical accuracy: One-step Euler method (error O(T²)) vs multi-step solving (error O(T⁴) or better)",
        "Sensitive dependence: Small changes in initial activation → different trajectories → different outputs",
        "Bifurcation analysis: Barrier function threshold defines bifurcation point in activation space",
        "Ergodicity: Unsteered activations may visit all regions (ergodic); steering introduces non-ergodic dynamics",
        "Phase space geometry: Activation space as high-dimensional manifold with complex attractor structure"
      ],
      "control_theory_connections": [
        "Barrier functions: Imported directly from control theory (Ames et al., 2016; 2019) for forward invariance",
        "Feedback vs open-loop: Traditional activation addition = open-loop control; ODESteer = closed-loop feedback control",
        "Lyapunov stability: Barrier function h(a) is Lyapunov function candidate; condition ḣ(a) > 0 is stability condition",
        "Gradient-based control: Steering direction v(a) = ∇h(a)/‖∇h(a)‖ is gradient-ascent control with normalization",
        "Gain scheduling: ODE solver implicitly implements gain scheduling based on activation region",
        "Robustness: Barrier functions provide robustness margins; system tolerates perturbations if h(a) remains positive",
        "Optimal control: Steering can be viewed as optimal control problem with cost function"
      ],
      "information_theory_connections": [
        "Log-density ratio: h(a) = log(p₊(a)/p₋(a)) related to pointwise KL divergence",
        "Information bottleneck: Polynomial Count Sketch performs random projection (information compression)",
        "Mutual information: Effective steering increases I(a;label) between activation and desired behavior",
        "Rate-distortion theory: Steering strength T trades rate (steering amount) vs distortion (behavior preservation)",
        "Entropy reduction: Steering reduces entropy of activation distribution, making model more predictable and aligned",
        "Fisher information: Gradient ∇h(a) related to Fisher information score; ODE follows natural gradient",
        "Discriminative modeling: Logistic regression learns optimal discrimination between classes"
      ],
      "brain_theory_connections": [
        "Predictive coding: Barrier function measures alignment error; steering minimizes prediction error",
        "Free energy principle: Steering reduces variational free energy by moving activations to high-probability (aligned) regions",
        "Hierarchical processing: ODESteer could be applied at multiple layers (hierarchical steering)",
        "Neural dynamics: LLM activations are neural representations; steering modulates neural dynamics",
        "Homeostatic regulation: Barrier functions define homeostatic boundaries for activations",
        "Attention as control: ODESteer modulates attentional focus by steering hidden states",
        "Neuromodulation: ODE steering analogous to neuromodulation in biological brains"
      ],
      "limitations": [
        "Computational cost: ODE solving is 2-5x slower than one-step activation addition",
        "Feature engineering: Relies on Polynomial Count Sketch with random projections and hyperparameters",
        "Layer selection: Applies steering at single layer; optimal layer selection remains open problem",
        "Dynamic environments: Evaluated on static benchmarks; no online adaptation analysis",
        "Safety guarantees: Barrier functions provide theoretical guarantees but no empirical robustness testing",
        "Scalability: Polynomial features scale quadratically; may limit application to very large models"
      ],
      "future_directions": [
        "Stability analysis: Prove convergence rates and stability margins for ODESteer",
        "Bifurcation analysis: Study how steering behavior changes as T (steering strength) varies",
        "Multi-layer steering: Apply ODESteer at multiple layers with coordinated control",
        "Adaptive integration: Use adaptive step-size solvers for better accuracy-efficiency tradeoff",
        "Feature learning: Learn optimal feature maps instead of fixed polynomial sketch",
        "Online updates: Continuously update barrier functions during deployment",
        "Robustness testing: Test against adversarial inputs and distribution shift",
        "Cross-disciplinary: Compare to neuromodulation in biological brains, motion planning in robotics"
      ],
      "analyzed": true,
      "analysis_file": "/home/devbox/project/paper-2602.17560-analysis.md",
      "reproduction_guide": "/home/devbox/project/paper-2602.17560-reproduction-guide.md",
      "pdf_file": "/home/devbox/project/paper-2602.17560.pdf",
      "analysis_date": "2026-02-22"
    },
    {
      "id": "2602.17547",
      "title": "KLong: Training LLM Agent for Extremely Long-horizon Tasks",
      "authors": [
        "Yue Liu",
        "et al."
      ],
      "submitted": "2026-02-19",
      "category": "LLM Training, Agent Architectures, Long-Horizon Tasks",
      "relevance": "CRITICAL - Novel framework for training LLM agents on extremely long-horizon tasks using trajectory-splitting SFT and progressive RL, demonstrating that better training outperforms larger models (106B beats 1T model by 11.28% on PaperBench)",
      "abstract_snippet": "This paper introduces KLong, an open-source LLM agent trained to solve extremely long-horizon tasks. The principle is to first cold-start the model via trajectory-splitting SFT, then scale it via progressive RL training. Specifically, we first activate basic agentic abilities of a base model with a comprehensive SFT recipe. Then, we introduce Research-Factory, an automated pipeline that generates high-quality training data by collecting research papers and constructing evaluation rubrics. Using this pipeline, we build thousands of long-horizon trajectories distilled from Claude 4.5 Sonnet (Thinking).",
      "notes": "KLong (106B) surpasses Kimi K2 Thinking (1T) by 11.28% on PaperBench. Generalizes to SWE-bench Verified and MLE-bench. Introduces three core innovations: (1) Trajectory-splitting SFT for handling arbitrarily long trajectories, (2) Research-Factory for automated data generation, (3) Progressive RL with stage-wise timeout extension. Strong connections to chaos theory (attractor dynamics, bifurcations, phase space structure, sensitive dependence), control theory (hierarchical control, feedback control, Lyapunov stability, model predictive control), information theory (information bottleneck, mutual information, rate-distortion theory, entropy reduction), and brain theory (predictive coding, hierarchical processing, working memory vs long-term memory, cognitive load theory).",
      "key_contributions": [
        "Trajectory-splitting SFT: Novel method for training on trajectories exceeding context windows with early context preservation, progressive truncation, and overlap maintenance",
        "Research-Factory: Automated pipeline for generating high-quality long-horizon training data from research papers with evaluation rubrics",
        "Progressive RL: Multi-stage RL training with progressively extended timeouts (Stage 1: 100 steps, Stage 2: 500 steps, Stage 3: 2000+ steps)",
        "PaperBench Results: KLong (106B) beats Kimi K2 Thinking (1T) by 11.28%, demonstrating better training > larger models",
        "Generalization: Performance improvements extend to SWE-bench Verified and MLE-bench benchmarks"
      ],
      "chaos_theory_connections": [
        "Attractor dynamics in trajectory space: Successful execution paths form high-dimensional attractors",
        "Trajectory-splitting as coarse-graining: Similar to coarse-grained descriptions of dynamical systems",
        "Overlaps as coupling regions: Overlapping sub-trajectories create coupling, analogous to coupled oscillators",
        "Sensitive dependence on initial conditions: Early context preservation critical because small changes in early decisions lead to vastly different outcomes (butterfly effect)",
        "Bifurcation analysis: Each RL stage represents a bifurcation point where policy behavior can diverge",
        "Timeout extension as bifurcation: Changing timeout parameter leads to qualitatively different policies",
        "Ergodicity and non-ergodicity: Long-horizon tasks exhibit non-ergodic behavior; progressive RL gradually explores more state space",
        "Phase space structure: Trajectory space as high-dimensional phase space where successful policies form attractor basins"
      ],
      "control_theory_connections": [
        "Hierarchical control with multiple time scales: Stage 1 (fast), Stage 2 (medium), Stage 3 (slow)",
        "High-level vs low-level control: Research-Factory provides high-level task specification; SFT and RL provide low-level execution",
        "Feedback control: RL updates based on reward signals (feedback); progressive stages use feedback from previous stages (reference tracking)",
        "Lyapunov stability: Progressive RL ensures stability by building on stable policies from previous stages",
        "Model Predictive Control (MPC): Trajectory-splitting SFT similar to MPC with receding horizon; each sub-trajectory like MPC planning window",
        "Adaptive control: Progressive RL adjusts timeout (control parameter) based on performance; curriculum learning adjusts task difficulty adaptively"
      ],
      "information_theory_connections": [
        "Information bottleneck: Trajectory-splitting SFT compresses long trajectories into manageable sub-trajectories; overlap preserves critical information while compressing",
        "Early context high MI: Early decisions have high mutual information with task outcome",
        "Later context low MI: Later steps have lower MI with final outcome (hence can be truncated)",
        "Overlap regions maintain MI: Preserve mutual information between adjacent sub-trajectories",
        "Rate-distortion theory: Training efficiency tradeoff - Rate (computational cost) vs Distortion (information loss from trajectory)",
        "Entropy reduction: Long trajectories have high entropy; successful policies reduce entropy by constraining behavior to successful attractor basins",
        "Information compression: Claude 4.5 Sonnet distillation compresses expert knowledge into training examples; SFT compresses expert demonstrations into model weights"
      ],
      "brain_theory_connections": [
        "Predictive coding: Trajectory-splitting SFT learns to predict next actions given context; early context preservation aligns with brain's prioritization of early sensory information",
        "Progressive truncation similar to recency bias: Attention weights decay over time (temporal attention)",
        "Hierarchical processing: Multi-stage RL analogous to brain's hierarchical processing - Stage 1 (prefrontal cortex, immediate decisions), Stage 2 (temporal lobe, 中期规划), Stage 3 (hippocampus, long-term memory)",
        "Working memory vs long-term memory: Trajectory-splitting SFT trains working memory for immediate action selection; progressive RL builds long-term memory for strategic planning",
        "Cognitive load theory: Progressive timeout extension gradually increases cognitive load as model learns; stage-wise training prevents cognitive overload",
        "Plasticity and consolidation: Stage 1 → Stage 2 → Stage 3 analogous to memory consolidation - Stage 1 (short-term potentiation, rapid learning), Stage 2 (intermediate consolidation), Stage 3 (long-term potentiation, stable memories)",
        "Attention mechanisms: Early context preservation similar to attention focusing on early important features",
        "Dopaminergic reward system: RL as dopamine; progressive stages provide different reward signals for different temporal scales - short-term rewards (phasic dopamine), long-term rewards (tonic dopamine)",
        "Meta-cognition: Research-Factory rubrics provide meta-cognitive evaluation criteria; models learn to assess their own progress (meta-learning)"
      ],
      "limitations": [
        "Dependence on expert models: Relies on Claude 4.5 Sonnet (Thinking) for trajectory distillation",
        "Computational cost: Training requires significant compute resources (estimated 2250 A100-hours for 106B model)",
        "Domain specificity: Focused on research/coding tasks; generalization to other domains untested",
        "Benchmark limitations: PaperBench, SWE-bench, MLE-bench may not capture all aspects of long-horizon reasoning",
        "Scalability questions: How does the approach scale to even longer horizons or more complex tasks?"
      ],
      "future_directions": [
        "Cross-domain application: Apply to robotics, scientific research, creative domains",
        "Automatic curriculum learning: Learn optimal timeout schedules automatically",
        "Multi-agent extensions: Extend to multi-agent long-horizon tasks",
        "Human-in-the-loop: Integrate human feedback into progressive RL",
        "Transfer learning: Investigate zero-shot or few-shot transfer to new domains",
        "Efficiency improvements: Reduce computational cost of trajectory-splitting SFT",
        "Theoretical analysis: Formal convergence guarantees for progressive RL",
        "Benchmark expansion: Create more diverse long-horizon benchmarks"
      ],
      "analyzed": true,
      "analysis_file": "/home/devbox/project/paper-2602.17547-analysis.md",
      "reproduction_guide": "/home/devbox/project/paper-2602.17547-reproduction-guide.md",
      "pdf_file": "/home/devbox/project/paper-2602.17547.pdf",
      "analysis_date": "2026-02-22"
    },
    {
      "id": "2602.17641",
      "title": "FAMOSE: A ReAct Approach to Automated Feature Discovery",
      "authors": [
        "Keith Burghardt",
        "et al."
      ],
      "submitted": "2026-02-19",
      "category": "Agent Architectures & Feature Engineering",
      "relevance": "HIGH - First application of ReAct paradigm to automated feature engineering for both classification and regression tasks, achieving SOTA performance (especially on datasets >10K instances with 0.23% ROC-AUC improvement) and 2.0% RMSE reduction",
      "abstract_snippet": "Feature engineering remains a critical yet challenging bottleneck in machine learning, particularly for tabular data, as identifying optimal features from an exponentially large feature space traditionally demands substantial domain expertise. To address this challenge, we introduce FAMOSE (Feature Augmentation and Optimal Selection agent), a novel framework that leverages ReAct paradigm to autonomously explore, generate, and refine features while integrating feature selection and evaluation tools within an agent architecture. To our knowledge, FAMOSE represents first application of an agentic ReAct framework to automated feature engineering, especially for both regression and classification tasks.",
      "notes": "Based on Smolagents framework. Uses ReAct paradigm with 20 rounds of feature discovery, 10 steps per round, 1% improvement target. Integrates mRMR for final feature selection. Achieves SOTA on large classification datasets (>10K instances, +0.23% ROC-AUC) and regression tasks (-2.0% RMSE). Strong connections to chaos theory (attractor dynamics, sensitive dependence, self-organization), information theory (information bottleneck, mutual information, entropy reduction), control theory (feedback control, Lyapunov stability, adaptive control), and brain theory (predictive coding, cognitive load theory, working memory vs long-term memory, metacognition).",
      "key_contributions": [
        "ReAct-based feature discovery: Iterative generation, evaluation, and refinement through agent interaction with data",
        "Comprehensive benchmarking: Evaluated on 27 datasets (20 classification + 7 regression) across multiple models (XGBoost, Random Forest, AutoGluon)",
        "SOTA performance: Matches or exceeds SOTA, especially on large datasets (>10K instances: +0.23% ROC-AUC, regression: -2.0% RMSE)",
        "Feature interpretability: LLM-based reasoning provides human-interpretable explanations for why features are useful",
        "Robustness across models: Features work well across XGBoost, Random Forest, and AutoGluon"
      ],
      "chaos_theory_connections": [
        "Attractor dynamics: Feature space as phase space, iterative search navigates toward high-dimensional attractors (high-performance feature regions)",
        "Sensitive dependence: Small early feature decisions lead to large performance differences; context window remembers what worked/failed",
        "Bifurcation analysis: Each round represents bifurcation point where strategy can diverge; early stopping after 6 rounds prevents instability",
        "Self-organization: Framework drives system toward low-entropy (high-understanding) states through iterative refinement",
        "Butterfly effect: Small perturbations in feature space (adding/removing features) cause large changes in model performance"
      ],
      "information_theory_connections": [
        "Information bottleneck: LLM context window records feature history (similar to few-shot learning), compressing 200 steps (20 rounds × 10 steps) into manageable representation",
        "Mutual information: Feature selection (mRMR) maximizes I(Feature; Target) while minimizing I(Feature₁; Feature₂)",
        "Entropy: Search reduces feature space entropy by focusing on high-information regions; conditional entropy H(NewFeature | Original + Selected)",
        "Rate-distortion theory: 20 rounds limit represents rate constraint; performance loss represents distortion; tradeoff between exploration cost and performance gain"
      ],
      "control_theory_connections": [
        "Closed-loop feedback control: Feature proposal → Performance evaluation → Feedback → Strategy adjustment → New proposal",
        "Negative feedback: Features not improving performance trigger strategy adjustment",
        "Lyapunov stability: 1% improvement target ensures stability; 6-round early stop condition ensures asymptotic stability",
        "Adaptive control: Dynamic per-round strategy adjustment; three parameters (1% goal, 10-step limit, 20-round limit) as different control gains",
        "Model Reference Adaptive Control (MRAC): Each round adjusts search strategy based on performance feedback",
        "Hierarchical control: High-level (ReAct agent for strategy), mid-level (feature evaluation for measurement), low-level (code execution for implementation)"
      ],
      "brain_theory_connections": [
        "Predictive coding: LLM reasoning (high-level) predicts which features might work; feature evaluation provides low-level prediction error signals",
        "Cognitive load theory: LLM context window as working memory; 10 steps per round prevents cognitive overload; 20 rounds progressive load increase",
        "Working memory vs long-term memory: ReAct process (immediate feature generation and evaluation) vs context window history (long-term memory of what worked)",
        "Memory consolidation: mRMR as consolidation process (selects most important features to final feature set)",
        "Metacognition: Agent monitors own reasoning (why this feature useful?), reflects on performance, and adjusts strategy",
        "Episodic memory: Context window records episodes (rounds) of feature discovery; retrieval and learning from past episodes"
      ],
      "limitations": [
        "ReAct framework computational cost: High token consumption due to agent chain-of-thought reasoning",
        "LLM quality dependence: Performs worse on cheaper/smaller LLMs (e.g., Llama 3.1-8B)",
        "Background knowledge dependence: Performance depends on LLM's pre-training; RAG could help for unfamiliar domains",
        "Multi-label classification: Not yet supported; requires minor modifications",
        "No theoretical convergence guarantees: Empirical success without formal proof of optimality"
      ],
      "future_directions": [
        "Efficiency improvements: Reduce token consumption, faster LLM inference",
        "Cross-domain generalization: RAG integration for task-specific background knowledge",
        "Multi-label extension: Extend framework for multi-label classification tasks",
        "Automatic hyperparameter tuning: Learn optimal 1% target, 10-step limit, 20-round limit",
        "Distributed computing: Parallelize feature generation and evaluation",
        "Online adaptation: Adapt to streaming data scenarios",
        "Alternative feature selection: Compare mRMR to Lasso, Elastic Net, etc.",
        "Theoretical analysis: Formal convergence proofs, stability bounds, optimality guarantees"
      ],
      "analyzed": true,
      "analysis_file": "/home/devbox/project/paper-2602.17641-analysis.md",
      "reproduction_guide": "/home/devbox/project/paper-2602.17641-reproduction-guide.md",
      "pdf_file": "/home/devbox/project/paper-2602.17641.pdf",
      "analysis_date": "2026-02-22"
    },
    {
      "id": "2602.17062",
      "title": "Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning",
      "authors": [
        "Yonghyeon Jo",
        "Sunwoo Lee",
        "Seungyul Han"
      ],
      "institution": "Graduate School of Artificial Intelligence, Ulsan National Institute of Science and Technology (UNIST)",
      "submitted": "2026-02-19",
      "conference": "ICLR 2026",
      "category": "Multi-Agent Reinforcement Learning (MARL), CTDE Paradigm, Value Decomposition",
      "relevance": "CRITICAL - Novel framework that learns multiple sub-value functions to retain alternative high-value actions, enabling rapid adaptation when the optimal action shifts during training, avoiding premature convergence to suboptimal policies in cooperative MARL environments",
      "abstract_snippet": "Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables Qtot to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance.",
      "notes": "S2Q consistently outperforms QMIX, WQMIX, DOP, FOP, PAC, RiskQ, MARR on SMAC-Hard+, GRF, and SMAC-Comm benchmarks. Key innovation: successive learning of K+1 sub-value functions (K=2 optimal), each capturing a distinct suboptimal action. When optimal action changes, S2Q can immediately leverage corresponding sub-value function. Uses Softmax-based behavior policy (T=0.1) to encourage exploration. Encoder-decoder architecture enables coordination during training; communication-free during evaluation. Theoretical guarantee (Theorem 4.1): with bounded rewards and sufficient suppression factor α, successive learning maintains K+1 distinct high-value actions. Strong connections to chaos theory (attractor dynamics, bifurcations, coupled oscillators, non-ergodic dynamics), control theory (negative feedback loops, Lyapunov stability, adaptive control, model predictive control), information theory (information bottleneck, mutual information, rate-distortion theory, entropy reduction), and brain theory (predictive coding, hierarchical processing, working memory vs long-term memory, dopaminergic reward system, attention mechanisms).",
      "key_contributions": [
        "Novel problem formulation: First systematic identification and solution of 'shifting optima' problem in MARL",
        "Theoretical foundation: Strong theoretical guarantee (Theorem 4.1) under bounded rewards and sufficient suppression",
        "Successive sub-value learning: Learns K+1 sub-value functions (Q_sub^0, ..., Q_sub^K) tracking optimal and K suboptimal actions",
        "Softmax-based behavior policy: Replaces ε-greedy with Softmax distribution over sub-value functions; enables coordinated exploration",
        "Encoder-decoder coordination: Latent representation z_t enables agents to synchronize sub-value function selection during training",
        "Communication-free evaluation: Q_sub^0 alone suffices at test time; fully decentralized execution",
        "Consistent improvements: Achieves 15-20% improvement over QMIX on SMAC-Hard+, 16-17% on GRF, 17.5% on SMAC-Comm"
      ],
      "chaos_theory_connections": [
        "Attractor dynamics in value function space: Optimal and suboptimal actions as distinct attractors; S2Q maintains K+1 attractors instead of converging to single attractor",
        "Attractor basin hopping: When value landscape shifts, S2Q can 'hop' between pre-existing attractors rather than re-converging from scratch",
        "Bifurcation analysis: Changing payoff structure represents bifurcation point where system's dynamics qualitatively change; S2Q tracks multiple modes across bifurcation points",
        "Coupled oscillator analogy: Each agent's policy as oscillator; coupling through joint action selection; latent representation z_t as shared phase all agents synchronize to",
        "Non-ergodic dynamics: S2Q introduces structured exploration (non-ergodic) focusing on promising regions; ε-greedy as ergodic baseline",
        "Sensitive dependence on initial conditions: Small early tracking errors lead to large differences in final adaptation; success of successive learning depends on α being sufficiently large"
      ],
      "control_theory_connections": [
        "Negative feedback loop: Extended loop includes sub-value selection → action execution → feedback → update Q_sub^k → update Q_tot → repeat",
        "Lyapunov stability under dynamics: S2Q maintains stability when value functions shift by having pre-computed backup policies (sub-value functions); Theorem 4.1 as stability guarantee",
        "Model Predictive Control (MPC): Successive learning as receding horizon; each sub-value function provides different lookahead into value landscape; online adaptation during training",
        "Adaptive control: Q* as reference model; sub-value functions adjust to match it; temperature T acts as control gain for exploration-exploitation balance",
        "Optimal control theory: Extends Bellman optimality to dynamic environments; S2Q achieves Bellman optimality even when value function changes by maintaining backup policies",
        "Hierarchical control: High-level (Q* predicts which suboptimal mode will be optimal), mid-level (Q_sub^k tracks specific modes), low-level (agent policies execute individual actions given sub-value function)",
        "Gain scheduling: Softmax distribution P_t implements implicit gain scheduling based on relative values of sub-value functions"
      ],
      "information_theory_connections": [
        "Information bottleneck principle: S2Q's multi-stream architecture maintains K+1 parallel channels instead of collapsing into single; each sub-value function as separate information channel",
        "Mutual information maximization: Softmax distribution P_t maximizes I(a_t; Q*) between selected action and global value function",
        "Rate-distortion tradeoff: Temperature T controls rate (exploration diversity) vs distortion (accuracy of current optimal action); T=0.1 achieves optimal balance",
        "Entropy reduction: Softmax-based sampling reduces exploration entropy over time; lower T (sharper distribution) = lower entropy (more focused exploitation)",
        "Channel capacity: Limited by K (number of sub-value functions) and T (temperature); allocates capacity among sub-value functions based on estimated values",
        "Conditional entropy: H(a_t | P_t) = -Σ_k P_k,t log P_k,t measures uncertainty in sub-value function selection; controls exploration vs exploitation",
        "Cross-entropy loss minimization: TD learning minimizes cross-entropy between predicted Q-values and TD targets; weighted TD learning prioritizes high-error samples"
      ],
      "brain_theory_connections": [
        "Predictive coding hierarchy: High-level (Q* predicts which suboptimal mode will be optimal), mid-level (Q_sub^k predicts specific joint action for each mode), low-level (agent policies execute individual actions given sub-value function)",
        "Working memory: Sub-value functions maintained in parallel similar to brain's working memory; hold multiple representations simultaneously for immediate access",
        "Long-term memory: Encoder-decoder latent representation z_t; analogous to consolidated memory; retrieved during coordination",
        "Hierarchical processing: Successive learning forms hierarchy (Q_sub^0 as global optimum → Q_sub^1 as second-best → Q_sub^2 as third-best); analogous to cortical hierarchy",
        "Information Integration Theory (IIT): S2Q has high Φ due to multiple distinct sub-value functions and coordination via communication; each Q_sub^k contributes to Φ through interaction with Q* and other sub-value functions",
        "Free energy principle: RL minimizes expected return (analogous to free energy); S2Q's free energy composition: accuracy term (maximizing Q*), complexity term (Softmax entropy), regularization term (suppression of previously identified actions)",
        "Dopaminergic reward system: TD error r_t + γQ^*(s') - Q(s,a) provides reward prediction error; analogous to phasic dopamine firing; Softmax distribution provides baseline activation for different strategies",
        "Attention mechanisms: Softmax as attention: P_t = Softmax(Q*/T) weights different sub-value functions; temperature T controls attention focus (sharp vs broad)",
        "Meta-cognition: Softmax-based selection as meta-cognitive decision about which sub-value function to use; adapts based on learned value estimates"
      ],
      "limitations": [
        "Computational overhead: Multiple sub-value functions increase memory and computation (though Appendix H.5 shows moderate overhead relative to performance gains)",
        "Hyperparameter sensitivity: Requires tuning K (number of sub-value functions) and T (temperature); reasonable values require minimal tuning",
        "Theoretical gaps: No convergence rate bounds; asymptotic behavior under non-stationary value functions not fully analyzed",
        "Partial observability: Encoder-decoder required for coordination during training; may not scale to very large teams",
        "Limited benchmark diversity: SMAC and GRF focus on coordination tasks; may not generalize to all MARL scenarios",
        "Partial observability: Encoder-decoder required for coordination during training; may not scale to very large teams"
      ],
      "future_directions": [
        "Dynamic K: Adaptively adjusting number of sub-value functions during training based on environment complexity",
        "Meta-learning of T: Learning optimal temperature schedule automatically",
        "Multi-objective extension: Optimizing not just for return but also for robustness to value shifts",
        "Hierarchical S2Q: Applying S2Q at different temporal scales (short-term vs long-term suboptimal actions)",
        "Theoretical extensions: Formal convergence analysis under non-stationary value functions; Lyapunov stability bounds",
        "Scalability improvements: Reducing computational overhead of encoder-decoder; approximate Softmax computation",
        "Cross-method generalization: Testing S2Q with other CTDE methods (VDN, QPLEX) and comparing effectiveness",
        "Real-world applications: Applying to robotics swarm control, autonomous vehicles, drone fleets"
      ],
      "analyzed": true,
      "analysis_file": "/home/devbox/project/paper-2602.17062-analysis.md",
      "reproduction_guide": "/home/devbox/project/paper-2602.17062-reproduction-guide.md",
      "analysis_date": "2026-02-22"
    },
    {
      "id": "2602.17544",
      "title": "Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability",
      "authors": [
        "Shashank Aggarwal",
        "et al."
      ],
      "submitted": "2026-02-19",
      "category": "Agent Architectures, Multi-Agent IR Systems, CoT Evaluation",
      "relevance": "CRITICAL - Introduces novel Thinker-Executor framework for evaluating CoT quality using reusability and verifiability metrics, revealing that accuracy benchmarks are blind spots for reasoning quality",
      "abstract_snippet": "In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusability measures how easily an Executor can reuse the Thinker's CoT. Verifiability measures how frequently an Executor can match the Thinker's answer using the CoT from Thinker.",
      "notes": "Key finding: Reusability and verifiability rankings do NOT correlate with accuracy rankings (Kendall's τ ≈ 0). Specialized reasoning models (DeepSeek-R1, Phi4-reasoning) often outperformed by general-purpose models (Llama, Gemma) on reusability/verifiability. Thinker-Executor framework decouples CoT generation from execution. Critical for multi-agent systems where cross-model CoT transfer is essential. Strong connections to chaos theory (attractor dynamics, coupled oscillators, information flow), information theory (information bottleneck, mutual information, entropy reduction), control theory (feedback control, robust control, model-agnostic reference), and brain theory (predictive coding, Theory of Mind, working memory, IIT).",
      "key_contributions": [
        "Thinker-Executor framework: Decouples CoT generation (Thinker) from execution (Executor)",
        "Reusability metric: Measures how easily Executor can reuse Thinker's CoT (persuasiveness, transferability)",
        "Verifiability metric: Measures how consistently Executor matches Thinker's answer using CoT (robustness)",
        "Blind spot revelation: Accuracy doesn't correlate with reusability/verifiability (Kendall's τ ≈ 0)",
        "Specialization paradox: General-purpose models often produce more transferable CoT than specialized reasoning models",
        "Thinker evaluation: 4 models vs 10 Executors committee across 5 benchmarks"
      ],
      "chaos_theory_connections": [
        "CoT as dynamical system trajectory: Each CoT is a path in high-dimensional embedding space to answer attractor",
        "Attractor dynamics: Correct answers are attractors; good CoT trajectories reliably converge to attractors across different systems",
        "Coupled oscillators: Thinker and Executor as coupled oscillators; reusability measures coupling strength",
        "Information flow in multi-agent systems: CoT as information transmission channel; reusability measures SNR (signal-to-noise ratio)",
        "Sensitive dependence: Small CoT perturbations can lead to large answer differences (butterfly effect)",
        "Attractor basin stability: High verifiability → stable attractor basins; low verifiability → unstable basins"
      ],
      "information_theory_connections": [
        "CoT as compressed reasoning trace: Lossy compression of internal reasoning process",
        "Information bottleneck: CoT must compress reasoning while preserving critical information for task completion",
        "Mutual information: I(CoT; Answer | Question) measures information transfer; reusability measures cross-model I(CoT; Answer)",
        "Cross-model information transfer: Reusability measures I(Thinker_CoT; Executor_Answer) for different executors",
        "Rate-distortion theory: CoT length (rate) vs. reasoning fidelity (distortion) tradeoff",
        "Entropy and uncertainty: H(Answer | Question) vs. H(Answer | Question + CoT); good CoT maximally reduces entropy",
        "Shannon information: Good CoT provides high I(Answer; CoT | Question); reusability measures this across decoders"
      ],
      "control_theory_connections": [
        "Thinker-Executor as feedback control system: Thinker provides reference trajectory (CoT), Executor follows as plant",
        "Reference tracking: Reusability measures how well executor tracks reference (CoT)",
        "Tracking error: Verifiability measures tracking stability across different executors",
        "Model-agnostic control (robust control): Reusable CoT as robust reference that works across different plants (executors)",
        "H-infinity control: Minimize worst-case tracking error across all possible executors",
        "Feedback control law: Executor's reasoning u(t) = f(CoT, internal_state, error_feedback); reusability when CoT dominates",
        "Adaptive control: Executors adapt to Thinker's CoT through learning; adaptation speed measured by verifiability"
      ],
      "brain_theory_connections": [
        "Predictive coding and CoT: CoT as hierarchical prediction sequence in predictive coding framework",
        "Working memory vs. long-term memory: CoT as externalized working memory shared between agents",
        "Theory of Mind and communication: Reusable CoT requires good Theory of Mind (understanding other agents' reasoning)",
        "Information Integration Theory (IIT): Reusable CoT increases integrated information Φ in multi-agent system",
        "Meta-cognition and self-reflection: CoT quality reflects meta-cognitive awareness of reasoning process",
        "Cognitive load: CoT length and complexity affect transferability (working memory constraints)",
        "Explicit vs. implicit reasoning: General-purpose models better at explicit, transferable reasoning; specialized models better at implicit, model-specific intuition"
      ],
      "limitations": [
        "Evaluation scope: Only 5 datasets, all reasoning-focused; no real-world IR tasks",
        "Limited model pool: 4 thinkers, 10 executors (no commercial models like GPT-4, Claude)",
        "Theoretical gaps: No formal analysis of Lyapunov stability, attractor dynamics, information-theoretic bounds",
        "Practical gaps: No real deployment evaluation, cost analysis, or security analysis",
        "Domain specificity: IR pipeline focus; unknown if results transfer to other domains",
        "No formal guarantees: No convergence proofs or optimality bounds for reusability/verifiability"
      ],
      "future_directions": [
        "Theoretical extensions: Compute Lyapunov exponents for CoT trajectories, analyze attractor basin structure, apply information bottleneck principle",
        "Empirical extensions: Expand benchmarks (coding, creative writing), include commercial models, real-world IR deployment",
        "Practical applications: Design multi-agent coordination protocols using reusability metrics, develop CoT verification methods",
        "Security analysis: Analyze corrupted CoT as adversarial attack vector, design robust verification methods",
        "Human-AI collaboration: Develop CoT explanation systems for humans, interactive reasoning tools",
        "Cross-disciplinary: Apply to coding research, creative writing, multi-modal reasoning"
      ],
      "analyzed": true,
      "analysis_file": "/home/devbox/project/paper-2602.17544-analysis.md",
      "reproduction_guide": "/home/devbox/project/paper-2602.17544-reproduction-guide.md",
      "pdf_file": "/home/devbox/project/paper-2602.17544.pdf",
      "analysis_date": "2026-02-22"
    },
    {
      "id": "2602.16429",
      "title": "TabAgent: A Framework for Replacing Agentic Generative Components with Tabular-Textual Classifiers",
      "authors": [
        "Ido Levy",
        "Eilam Shapira",
        "Yinon Goldshtein",
        "Avi Yaeli",
        "Nir Mashkif",
        "Segev Shlomov"
      ],
      "submitted": "2026-02-18",
      "category": "Agent Architectures, LLM Inference Optimization",
      "relevance": "CRITICAL - Paradigm shift: replaces generative LLM components with discriminative classifiers for closed-set selection tasks, achieving 95% latency reduction and 85-91% cost reduction while maintaining task-level success",
      "abstract_snippet": "Agentic systems, AI architectures that autonomously execute multi-step workflows to achieve complex goals, are often built using repeated large language model (LLM) calls for closed-set decision tasks such as routing, shortlisting, gating, and verification. While convenient, this design makes deployments slow and expensive due to cumulative latency and token usage. We propose TabAgent, a framework for replacing generative decision components in closed-set selection tasks with a compact textual-tabular classifier trained on execution traces. TabAgent (i) extracts structured schema, state, and dependency features from trajectories (TabSchema), (ii) augments coverage with schema-aligned synthetic supervision (TabSynth), and (iii) scores candidates with a lightweight classifier (TabHead). On the long-horizon AppWorld benchmark, TabAgent maintains task-level success while eliminating shortlist-time LLM calls, reducing latency by approximately 95% and inference cost by 85-91%. Beyond tool shortlisting, TabAgent generalizes to other agentic decision heads, establishing a paradigm for learned discriminative replacements of generative bottlenecks in production agent architectures.",
      "notes": "TabAgent achieves order-of-magnitude improvements in latency (95%) and cost (85-91%) without sacrificing task success. Introduces three-component framework: TabSchema (schema extraction), TabSynth (synthetic supervision), TabHead (lightweight classifier). Strong connections to chaos theory (attractor dynamics, entropy reduction, sensitive dependence), information theory (information bottleneck, rate-distortion theory, mutual information, cross-entropy), control theory (feedback control, Lyapunov stability, hierarchical control, model-reference adaptive control), and brain theory (predictive coding, working memory vs long-term memory, dopamine reward system, IIT, consciousness vs automation).",
      "key_contributions": [
        "Paradigm shift: Generative → Discriminative for closed-set decisions",
        "TabSchema: Structured feature extraction from trajectories",
        "TabSynth: Schema-aligned synthetic supervision generation",
        "TabHead: Lightweight classifier for fast candidate scoring",
        "Empirical validation: AppWorld benchmark with 95% latency, 85-91% cost reduction",
        "Generalizability: Framework applies beyond tool shortlisting to other decision heads"
      ],
      "chaos_theory_connections": [
        "Decision space as high-dimensional dynamical system with stable attractors",
        "TabHead learns basin of attraction for each candidate",
        "Entropy reduction: Generative (high entropy) → Discriminative (low entropy) = efficiency gain",
        "Sensitive dependence: TabSchema extracts critical features determining outcomes",
        "Attractor basins: Closed-set tasks have narrow attractor basins, not open exploration"
        "Bifurcation points: TabSynth explores bifurcation points systematically"
        "Deterministic output: Classifiers produce deterministic, stable outputs vs stochastic LLM generation"
      ],
      "information_theory_connections": [
        "Information bottleneck: TabSchema compresses trajectory information",
        "Rate-distortion tradeoff: TabHead minimizes compute (rate) while maintaining accuracy (low distortion)",
        "Mutual information: I(features; correct_decision) maximized",
        "Cross-entropy: TabHead trained with cross-entropy loss, minimizing KL divergence",
        "95% latency reduction = 95% rate reduction (information-theoretic efficiency)",
        "Synthetic supervision: TabSynth increases I(features; decision) coverage"
      ],
      "control_theory_connections": [
        "Feedback control loop: State → Decision (TabHead) → Action → Update state",
        "Lyapunov stability: TabHead provides deterministic, stable decision function",
        "Hierarchical control: High-level (LLM planner) vs Low-level (TabHead controller)",
        "Model-reference adaptive control: LLM (reference) vs TabHead (adapted) model",
        "Separation of concerns: Planning (creative) vs Decision (fast, automatic)"
        "Gain scheduling: Multiple TabHeads as different control gains"
      ],
      "brain_theory_connections": [
        "Predictive coding hierarchy: LLM generates schema (high-level) → TabHead predicts action (low-level)",
        "Working memory: TabSchema extracts current state → fast, limited capacity (prefrontal cortex)",
        "Long-term memory: TabSynth accumulates synthetic examples → slow, high capacity (hippocampus to neocortex)",
        "Memory consolidation: Observe → Extract → Generate → Train (rehearsal → consolidation)",
        "Dopaminergic reward system: Minimize decision error (proxy for reward)",
        "Habit formation: TabHead = automatic, fast; LLM = conscious, slow",
        "Information Integration Theory (IIT): Integrated system (TabSchema + TabSynth + TabHead) with high Φ",
        "Consciousness vs automation: LLM (high Φ, conscious) vs TabHead (lower Φ, automatic)"
      ],
      "limitations": [
        "Closed-set assumption: Works best for closed-set selection, not open-ended decisions",
        "Training data requirement: Requires execution traces for TabSchema extraction",
        "Schema evolution: May need retraining if tool schemas change",
        "Generalization unknown: Single benchmark (AppWorld), domain specificity unclear",
        "Feature quality: TabSchema quality determines TabHead performance",
        "Limited evaluation: A/B testing mentioned but detailed results not shown"
      ],
      "future_directions": [
        "Automatic closed-set detection: Learn to identify closed-set vs open-ended decisions",
        "Incremental learning: Update TabHead online with new trajectories",
        "Multi-modal TabAgent: Extend beyond textual agents (vision, audio, code)",
        "Theoretical analysis: Formalize when discriminative is optimal (information-theoretic bounds)",
        "Brain-inspired: Integrate neuromodulation, memory replay mechanisms",
        "Multi-agent coordination: Multiple TabHeads with coordination mechanisms",
        "Cross-domain generalization: Test on diverse benchmarks beyond AppWorld",
        "Hybrid architectures: LLM + TabHead with automatic routing"
      ],
      "analyzed": true,
      "analysis_file": "/home/devbox/project/paper-2602.16429-analysis.md",
      "reproduction_guide": "/home/devbox/project/paper-2602.16429-reproduction-guide.md",
      "pdf_file": "/home/devbox/project/paper-2602.16429.pdf",
      "analysis_date": "2026-02-23"
    },
    {
      "id": "2602.16902",
      "title": "LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs",
      "authors": [
        "William Bankes",
        "Lorenz Wolf",
        "Shyam Sundhar Ramesh",
        "Xiaohang Tang",
        "Ilija Bogunovic"
      ],
      "submitted": "2026-02-18",
      "category": "Agent Architectures, Long-horizon Planning, Knowledge Graphs",
      "relevance": "HIGH - Evaluates planning and reasoning capabilities essential for agent architectures through interactive WikiRace game on Wikipedia hyperlink graph, revealing that even the strongest models (Gemini-3, GPT-5, Claude Opus 4.5) achieve only 23% success rate on hard difficulty",
      "abstract_snippet": "We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest results on the easy level of the task and demonstrate superhuman performance. Despite this, performance drops sharply on hard difficulty: the best-performing model, Gemini-3, succeeds in only 23% of hard games, highlighting substantial remaining challenges for frontier models.",
      "notes": "Evaluates 20+ frontier models across 3 difficulty levels. Reveals 'planning gap': models with similar world knowledge diverge in planning performance. Identifies loop behavior as systematic failure mode: models encounter loops in 61-86% of trajectories, aware they're stuck but fail to escape. Strong negative correlation (-1.02) between loop frequency and success rate. Key insight: world knowledge is necessary but insufficient for long-horizon planning. Strong connections to chaos theory (attractor dynamics, limit cycles, sensitive dependence on initial conditions, non-ergodic behavior), information theory (information bottleneck, mutual information, entropy reduction, channel capacity), control theory (feedback control, Lyapunov stability, model predictive control, adaptive control), and brain theory (predictive coding, hierarchical processing, state tracking, exploration-exploitation).",
      "key_contributions": [
        "Open-domain planning benchmark: LLM-WikiRace evaluates planning over large real-world knowledge graph (Wikipedia)",
        "Large-scale evaluation: 20+ models (Gemini 3, GPT-5, Claude Opus 4.5, DeepSeek R1, Kimi K2, LLaMA, Gemma, etc.)",
        "Planning gap identification: World knowledge necessary but not sufficient; planning ability becomes dominant factor",
        "Behavioral analysis: Reveals hub-seeking strategy, loop behavior (61-86% frequency), replanning failures",
        "Performance stratification: Clear differentiation by difficulty (Easy >90%, Medium 50-70%, Hard <25%)",
        "Human baseline comparison: LLMs achieve 100% success vs human 98.5% on human gameplay corpus, with fewer suboptimal steps"
      ],
      "chaos_theory_connections": [
        "Attractor dynamics in trajectory space: Each target page represents attractor basin; successful paths converge rapidly",
        "Limit cycles as failure mode: Loops represent persistent non-convergent trajectories; models acknowledge being stuck but don't escape",
        "Sensitive dependence on initial conditions: Poor early link choices cause irreversible divergence; hub-seeking critical early in episode",
        "Non-ergodic behavior: Hard tasks show non-ergodic dynamics (trapped in limited state subset)",
        "Bifurcation points in planning strategy: Awareness without adaptation represents failure to bifurcate to alternative strategies",
        "Butterfly effect: Small early decisions amplify through limited step budget to large final errors",
        "Information flow and entropy: Successful navigation monotonically reduces entropy; loops represent entropy plateaus"
      ],
      "information_theory_connections": [
        "Information bottleneck principle: Planning as compression under rate-distortion constraints (50-link filter, 30-step budget)",
        "Mutual information I(current; target): Planning approximates MI maximization; hub pages as high-MI states",
        "Channel capacity: 30-step budget limits information transmission; hard tasks approach capacity limit",
        "Predictive coding and forward reasoning: Hierarchical predictions (navigate to broad topic, then specific location); error minimization framework",
        "Shannon entropy in planning: Many locally plausible actions create high entropy; successful planning reduces H(target | current, history)"
      ],
      "control_theory_connections": [
        "Feedback control system: Reference (target) → Controller (LLM) → Plant (Wikipedia graph) → Output (navigation path)",
        "Model Predictive Control (MPC): Forward planning approximates MPC with receding horizon but limited lookahead",
        "Lyapunov stability: Loops represent unstable limit cycles; replanning policies lack Lyapunov stability",
        "Adaptive control failures: Fixed-gain controllers (constant hub-seeking) fail adaptive requirements of hard tasks",
        "Reference tracking error: Deviations from target not effectively used to adjust strategy",
        "Control authority limited: 30-step budget constrains control authority, preventing convergence on hard tasks"
      ],
      "brain_theory_connections": [
        "State tracking and memory: Limited state tracking (visits set) and poor cross-episode memory",
        "Exploration vs exploitation balance: Over-committed to exploitation (hub-seeking), under-explores alternatives",
        "Hierarchical processing: Hub-seeking (high-level) vs link selection (low-level) but poor integration",
        "Meta-cognition: Models monitor own behavior but fail to use meta-cognitive signals for adaptation",
        "Working memory limitations: Limited context window for maintaining alternative strategies",
        "Dopaminergic reward analogy: Successful navigation as reward prediction; replanning failures as prediction errors"
      ],
      "limitations": [
        "Benchmark limitations: Fixed Wikipedia snapshot (June 2025), 50-link filter may remove optimal paths, 30-step budget may not be optimal",
        "Model evaluation limitations: Only Gemini 3 and Claude Opus 4.5 provide explicit reasoning traces",
        "No learning across episodes: Each game independent; models don't learn from past failures",
        "Theoretical gaps: No formal stability analysis, planning gap mechanism unexplained, loop dynamics not characterized",
        "Fine-tuning limitations: DAPO fine-tuning improves easy but not hard tasks; one-step reduction of RL problem",
        "Domain specificity: Wikipedia navigation may not generalize to other knowledge graphs"
      ],
      "future_directions": [
        "Improving planning capabilities: Deeper lookahead, explicit replanning mechanisms, tree search integration",
        "Memory and learning: Cross-episode memory, graph learning, strategy libraries",
        "Adaptive exploration: Dynamic exploration policies, difficulty-dependent exploration, uncertainty estimation",
        "Benchmark extensions: Broader domains, longer horizons, time constraints, multi-agent scenarios",
        "Theoretical analysis: Formal stability proofs, loop dynamics characterization, information-theoretic bounds",
        "Agent architecture improvements: Hierarchical planning, explicit memory mechanisms, better exploration-exploitation balance"
      ],
      "analyzed": true,
      "analysis_file": "/home/devbox/project/paper-2602.16902-analysis.md",
      "reproduction_guide": "/home/devbox/project/paper-2602.16902-reproduction-guide.md",
      "pdf_file": "/home/devbox/project/paper-2602.16902.pdf",
      "analysis_date": "2026-02-23"
    }
  ],
  "last_updated": "2026-02-23T01:14:00Z"
}
